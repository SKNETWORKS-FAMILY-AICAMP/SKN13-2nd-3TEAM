import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os

from util.model_io import load_models
from util.visualizer import (
    plot_confusion_matrix,
    plot_roc_curve,
    plot_precision_recall_curve,
    plot_model_performance_comparison
)

# ------------------ ê²½ë¡œ ì„¤ì • ------------------
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_PATH = os.path.join(ROOT_DIR, "model", "all_models.pkl")

# ëª¨ë¸ + í‰ê°€ ì§€í‘œ ë¶ˆëŸ¬ì˜¤ê¸° (ì „ì²´ í˜ì´ì§€ì—ì„œ ì‚¬ìš©)
model_bundle = load_models(MODEL_PATH)

# ------------------ í˜ì´ì§€ ì„¤ì • ------------------
st.set_page_config(page_title="ì˜ˆì¸¡ëª¨ë¸ ì„±ëŠ¥ í‰ê°€", layout="wide")
st.title("ğŸ“Š ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ëŒ€ì‹œë³´ë“œ")

# ------------------ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ë° ì„ íƒ UI ------------------
model_names = list(model_bundle.keys())
model_options = ['ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”'] + model_names
selected_model = st.selectbox("ğŸ”½ í‰ê°€í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”", model_options)

# ------------------ ê¸°ë³¸ê°’ ì„ íƒ: ì „ì²´ ì„±ëŠ¥ ìš”ì•½ ------------------
if selected_model == "ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”":
    st.markdown("### âœ… ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")

    # ğŸ“Š í‰ê°€ ì§€í‘œë§Œ ì¶”ì¶œ
    metrics_data = []
    for model_name, info in model_bundle.items():
        m = info['metrics']
        metrics_data.append({
            "Model": model_name,
            "Accuracy": m["test_accuracy"],
            "Precision": m["classification_report"]["1"]["precision"],
            "Recall": m["classification_report"]["1"]["recall"],
            "F1-score": m["classification_report"]["1"]["f1-score"],
            "ROC-AUC": m.get("roc_auc", np.nan)
        })

    results_df = pd.DataFrame(metrics_data).sort_values(by="Accuracy", ascending=False)

    # ğŸ“‹ í‘œ í˜•íƒœ ì¶œë ¥
    st.dataframe(results_df, use_container_width=True)

    # ğŸ“ˆ ì‹œê°í™” ì¶œë ¥
    st.markdown("#### ğŸ“‰ Accuracy ê¸°ë°˜ ì„±ëŠ¥ ë¹„êµ (ìˆ˜í‰ ë§‰ëŒ€ ì°¨íŠ¸)")
    fig = plot_model_performance_comparison(results_df[["Model", "Accuracy"]])
    st.pyplot(fig)

    st.caption("ğŸ” ì™¼ìª½ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•˜ë©´, ì•„ë˜ì— í•´ë‹¹ ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€ ê·¸ë˜í”„ê°€ í‘œì‹œë©ë‹ˆë‹¤.")

# ------------------ ëª¨ë¸ ì„ íƒ ì‹œ: ì„±ëŠ¥ ì‹œê°í™” ------------------
else:
    st.markdown(f"### ğŸ” {selected_model} ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™”")

    model_info = model_bundle[selected_model]
    model = model_info["model"]
    metrics = model_info["metrics"]
    y_test = metrics["y_test"]
    y_pred = model.predict(metrics["X_test"])
    y_proba = model.predict_proba(metrics["X_test"])[:, 1]

    col1, col2 = st.columns(2)

    with col1:
        fig1 = plot_confusion_matrix(y_test, y_pred, title=f"Confusion Matrix - {selected_model}")
        st.pyplot(fig1)

    with col2:
        fig2 = plot_roc_curve(y_test, y_proba, estimator_name=selected_model, title="ROC Curve")
        st.pyplot(fig2)

        fig3 = plot_precision_recall_curve(y_test, y_proba, estimator_name=selected_model, title="Precision-Recall Curve")
        st.pyplot(fig3)

    st.caption("â€» ìœ„ ê²°ê³¼ëŠ” ì„ íƒí•œ ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ì— ê¸°ë°˜í•©ë‹ˆë‹¤.")
