import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os

from util.model_io import load_models
from util.visualizer import (
    plot_confusion_matrix,
    plot_roc_curve,
    plot_precision_recall_curve,
    plot_model_performance_comparison,
    plot_prediction_histogram
)

# ------------------ ê²½ë¡œ ì„¤ì • ------------------
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_PATH = os.path.join(ROOT_DIR, "model", "all_models.pkl")

# ëª¨ë¸ + í‰ê°€ ì§€í‘œ ë¶ˆëŸ¬ì˜¤ê¸° (ì „ì²´ í˜ì´ì§€ì—ì„œ ì‚¬ìš©)
model_bundle = load_models(MODEL_PATH)

# ------------------ í˜ì´ì§€ ì„¤ì • ------------------
st.set_page_config(page_title="ì˜ˆì¸¡ëª¨ë¸ ì„±ëŠ¥ í‰ê°€", layout="wide")
st.title("ğŸ“Š ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ëŒ€ì‹œë³´ë“œ")

# ------------------ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ë° ì„ íƒ UI ------------------
model_names = list(model_bundle.keys())
model_options = ['ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”'] + model_names
selected_model = st.selectbox("ğŸ”½ í‰ê°€í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”", model_options)

# ------------------ ê¸°ë³¸ê°’ ì„ íƒ: ì „ì²´ ì„±ëŠ¥ ìš”ì•½ ------------------
if selected_model == "ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”":
    st.markdown("### âœ… ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")

    # ğŸ“Š í‰ê°€ ì§€í‘œë§Œ ì¶”ì¶œ
    metrics_data = []
    for model_name, info in model_bundle.items():
        m = info['metrics']
        metrics_data.append({
            "Model": model_name,
            "Test Accuracy": m["test_accuracy"],
            "Precision": m["classification_report"]["macro avg"]["precision"],
            "Recall": m["classification_report"]["macro avg"]["recall"],
            "F1-score": m["classification_report"]["macro avg"]["f1-score"],
            "ROC-AUC": m.get("roc_auc", np.nan)
        })

    results_df = pd.DataFrame(metrics_data).sort_values(by="Test Accuracy", ascending=False)

    # ğŸ“‹ í‘œ í˜•íƒœ ì¶œë ¥
    st.dataframe(results_df, use_container_width=True)

    # ğŸ“ˆ ì‹œê°í™” ì¶œë ¥
    st.markdown("#### ğŸ“‰ Accuracy ê¸°ë°˜ ì„±ëŠ¥ ë¹„êµ (ìˆ˜í‰ ë§‰ëŒ€ ì°¨íŠ¸)")
    fig = plot_model_performance_comparison(results_df)
    st.pyplot(fig)

    st.caption("ğŸ” ì™¼ìª½ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•˜ë©´, ì•„ë˜ì— í•´ë‹¹ ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€ ê·¸ë˜í”„ê°€ í‘œì‹œë©ë‹ˆë‹¤.")

# ------------------ ëª¨ë¸ ì„ íƒ ì‹œ: ì„±ëŠ¥ ì‹œê°í™” ------------------
else:
    st.markdown(f"### ğŸ” {selected_model} ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™”")

    model_info = model_bundle[selected_model]
    model = model_info["model"]
    metrics = model_info["metrics"]
    y_test = metrics["y_test"]
    y_pred = model.predict(metrics["X_test"])
    y_proba = model.predict_proba(metrics["X_test"])[:, 1]

    # ìƒë‹¨: Confusion Matrix (ì¢Œ) + ROC Curve (ìš°)
    top1, top2 = st.columns([1, 1])

    with top1:
        fig1 = plot_confusion_matrix(y_test, y_pred, title=f"Confusion Matrix - {selected_model}")
        st.pyplot(fig1, use_container_width=True)

    with top2:
        if y_proba is not None:
            fig2 = plot_roc_curve(y_test, y_proba, estimator_name=selected_model, title="ROC Curve")
            st.pyplot(fig2, use_container_width=True)
        else:
            st.warning("âš  ROC Curve ë¯¸ì§€ì›")

    # í•˜ë‹¨: PR Curve + F1 Curve
    bottom1, bottom2 = st.columns([1, 1])

    with bottom1:
            fig3 = plot_precision_recall_curve(y_test, y_proba, estimator_name=selected_model, title="Precision-Recall Curve")
            st.pyplot(fig3, use_container_width=True)

    with bottom2:
            st.markdown("### ğŸ“Š Prediction Distribution")
            fig5 = plot_prediction_histogram(y_test, y_pred)
            st.pyplot(fig5, use_container_width=True)

