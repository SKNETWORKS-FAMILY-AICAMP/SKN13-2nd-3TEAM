
import math
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay, 
    recall_score, precision_score, f1_score, accuracy_score,
    PrecisionRecallDisplay, average_precision_score, precision_recall_curve,
    RocCurveDisplay, roc_auc_score, roc_curve, mean_squared_error, mean_absolute_error, r2_score
)

__version__ = 1.1

######################################
# ğŸ“Š ëª¨ë¸ í‰ê°€ ì§€í‘œ ì‹œê°í™” í•¨ìˆ˜
######################################

def plot_model_performance_comparison(results_df, figsize=(10, 6)):
    fig, ax = plt.subplots(figsize=figsize)
    bars = ax.barh(results_df['Model'], results_df['Accuracy'], color='skyblue')
    ax.set_xlabel('Test Accuracy')
    ax.set_title('Model Performance Comparison')
    ax.set_xlim(0, 1.0)

    # ì •í™•ë„ ê°’ í‘œì‹œ
    for bar in bars:
        width = bar.get_width()
        ax.text(width + 0.01, bar.get_y() + bar.get_height()/2, f'{width:.3f}', 
                ha='left', va='center')

    plt.tight_layout()
    plt.show()
    return fig


def plot_confusion_matrix(y_true, y_pred, title=None):
    cm = confusion_matrix(y_true, y_pred)
    fig, ax = plt.subplots()
    sns.heatmap(cm, annot=True, fmt="d", ax=ax, cmap="Blues")
    ax.set_title(title or "Confusion Matrix")
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")
    plt.tight_layout()
    plt.show()
    return fig

def plot_roc_curve(y_true, y_scores, estimator_name=None, title=None):
    auc_score = roc_auc_score(y_true, y_scores)
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    fig, ax = plt.subplots()
    disp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc_score, estimator_name=estimator_name)
    disp.plot(ax=ax)
    ax.set_title(title or "ROC Curve")
    plt.tight_layout()
    plt.show()
    return fig

def plot_precision_recall_curve(y_true, y_scores, estimator_name=None, title=None):
    ap = average_precision_score(y_true, y_scores)
    precision, recall, _ = precision_recall_curve(y_true, y_scores)
    fig, ax = plt.subplots()
    disp = PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=ap, estimator_name=estimator_name)
    disp.plot(ax=ax)
    ax.set_title(title or "Precision-Recall Curve")
    plt.tight_layout()
    plt.show()
    return fig


def plot_predicted_vs_actual(y_true, y_pred, title="Predicted vs Actual"):
    """
    ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ë¹„êµí•˜ëŠ” ì‚°ì ë„ ê·¸ë˜í”„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    ì˜ˆì¸¡ê°’ì´ ì •í™•í• ìˆ˜ë¡ ì ë“¤ì´ ëŒ€ê°ì„ (45ë„ ê¸°ì¤€ì„ )ì„ ë”°ë¼ ë¶„í¬í•˜ê²Œ ë©ë‹ˆë‹¤.

    Parameters:
    - y_true: ì‹¤ì œ ê°’ ë°°ì—´
    - y_pred: ì˜ˆì¸¡ ê°’ ë°°ì—´
    - title: ê·¸ë˜í”„ ì œëª©

    Returns:
    - fig: matplotlib figure ê°ì²´ (streamlitì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
    """
    fig, ax = plt.subplots()
    ax.scatter(y_true, y_pred, alpha=0.6)
    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')  # 45ë„ ê¸°ì¤€ì„ 
    ax.set_xlabel("Actual")
    ax.set_ylabel("Predicted")
    ax.set_title(title)
    plt.tight_layout()
    plt.show()
    return fig

def plot_residuals(y_true, y_pred, title="Residual Plot"):
    """
    ì”ì°¨(residual = ì‹¤ì œê°’ - ì˜ˆì¸¡ê°’)ë¥¼ ì‹œê°í™”í•˜ì—¬ 
    ì˜ˆì¸¡ ëª¨ë¸ì´ í¸í–¥(bias)ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    ì´ìƒì ìœ¼ë¡œëŠ” ì”ì°¨ë“¤ì´ 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ê³ ë¥´ê²Œ ë¶„í¬í•´ì•¼ í•©ë‹ˆë‹¤.

    Parameters:
    - y_true: ì‹¤ì œ ê°’ ë°°ì—´
    - y_pred: ì˜ˆì¸¡ ê°’ ë°°ì—´
    - title: ê·¸ë˜í”„ ì œëª©

    Returns:
    - fig: matplotlib figure ê°ì²´
    """
    residuals = y_true - y_pred
    fig, ax = plt.subplots()
    ax.scatter(y_pred, residuals, alpha=0.6)
    ax.axhline(0, color='red', linestyle='--')  # ê¸°ì¤€ì„ 
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Residuals")
    ax.set_title(title)
    plt.tight_layout()
    plt.show()
    return fig

def plot_prediction_histogram(y_true, y_pred, title="Prediction Distribution"):
    """
    ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ë¶„í¬ë¥¼ íˆìŠ¤í† ê·¸ë¨ + KDE(ë°€ë„ê³¡ì„ ) í˜•íƒœë¡œ ë¹„êµí•©ë‹ˆë‹¤.
    ë‘ ë¶„í¬ê°€ ë¹„ìŠ·í• ìˆ˜ë¡ ëª¨ë¸ì´ ì˜ ì˜ˆì¸¡í•˜ê³  ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.

    Parameters:
    - y_true: ì‹¤ì œ ê°’ ë°°ì—´
    - y_pred: ì˜ˆì¸¡ ê°’ ë°°ì—´
    - title: ê·¸ë˜í”„ ì œëª©

    Returns:
    - fig: matplotlib figure ê°ì²´
    """
    fig, ax = plt.subplots()
    sns.histplot(y_true, color='blue', label='Actual', kde=True, stat="density", ax=ax)
    sns.histplot(y_pred, color='orange', label='Predicted', kde=True, stat="density", ax=ax)
    ax.set_title(title)
    ax.legend()
    plt.tight_layout()
    plt.show()
    return fig

def regression_metrics_table(y_true, y_pred):
    """
    íšŒê·€ í‰ê°€ ì§€í‘œë“¤ì„ ê³„ì‚°í•˜ì—¬ dictë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    Streamlitì—ì„œëŠ” st.table() ë“±ìœ¼ë¡œ ì¶œë ¥ ê°€ëŠ¥.

    í¬í•¨ëœ ì§€í‘œ:
    - MAE: í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (Mean Absolute Error)
    - MSE: í‰ê·  ì œê³± ì˜¤ì°¨ (Mean Squared Error)
    - RMSE: í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨ (Root MSE)
    - R2 Score: ê²°ì • ê³„ìˆ˜ (ì„¤ëª…ë ¥)

    Parameters:
    - y_true: ì‹¤ì œ ê°’ ë°°ì—´
    - y_pred: ì˜ˆì¸¡ ê°’ ë°°ì—´

    Returns:
    - dict: ê° ì§€í‘œë“¤ì˜ ëª…ì¹­ê³¼ ê°’
    """
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return {
        "MAE": round(mae, 4),
        "MSE": round(mse, 4),
        "RMSE": round(rmse, 4),
        "R2 Score": round(r2, 4)
    }


######################################
# ğŸ“ˆ EDA ì‹œê°í™” í•¨ìˆ˜
######################################

def plot_boxplot_by_target(df, target_col='target', numeric_cols=None, title_prefix="Box Plot by", color_palette='Set2'):
    """
    target ê°’(ì´ì§„ ë¶„ë¥˜)ì— ë”°ë¼ ê·¸ë£¹í™”ëœ boxplotì„ ì»¬ëŸ¼ë³„ë¡œ ì‹œê°í™”

    Parameters:
    - df: DataFrame
    - target_col: str - íƒ€ê²Ÿ ì»¬ëŸ¼ëª… (ì˜ˆ: 'target' ë˜ëŠ” 'churn')
    - numeric_cols: list[str] - ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: ìˆ˜ì¹˜í˜• ìë™ ì„ íƒ)
    - title_prefix: str - ê·¸ë˜í”„ ì œëª© ì•ë¶€ë¶„
    - color_palette: str - seaborn color palette (ê¸°ë³¸ê°’: Set2)
    """
    if numeric_cols is None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.drop(target_col).tolist()

    for col in numeric_cols:
        plt.figure(figsize=(6, 4))
        sns.boxplot(data=df, x=target_col, y=col, palette=color_palette, showmeans=True,
                    meanprops={"marker": "o", "markerfacecolor": "white", "markeredgecolor": "black"})
        plt.title(f"{title_prefix} '{col}' by {target_col}", fontsize=13)
        plt.xlabel(target_col)
        plt.ylabel(col)
        plt.grid(axis='y', linestyle='--', alpha=0.5)
        plt.tight_layout()
        plt.show()

    #plt.savefig("my_plot.png")


# numeric_cols ë¦¬ìŠ¤íŠ¸ì— ë”°ë¼ ìƒê´€ê³„ìˆ˜ íˆíŠ¸ë§µì„ ì‹œê°í™”
def plot_correlation_heatmap(df, numeric_cols, target_col="target", title="íˆíŠ¸ë§µ ì‹œê°í™”ë¥¼ í†µí•œ ì»¬ëŸ¼ë³„ ìƒê´€ê´€ê³„ í™•ì¸", annot=True, cmap='coolwarm', figsize=(12, 10)):
    """
    numeric_cols ë¦¬ìŠ¤íŠ¸ì— ë”°ë¼ ìƒê´€ê³„ìˆ˜ íˆíŠ¸ë§µì„ ì‹œê°í™”
    """
    corr = df[numeric_cols + [target_col]].corr()
    plt.figure(figsize=figsize)
    sns.heatmap(corr, annot=annot, fmt=".4f", cmap=cmap, linewidths=0.5, vmin=-1, vmax=1, cbar_kws={"label": "Correlation"})
    plt.title(title, fontsize=14, fontweight='bold')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

#  ì—¬ëŸ¬ ì»¬ëŸ¼ì— ëŒ€í•´ ì´íƒˆì(target=1) ë¶„í¬ë¥¼ ê·¸ë£¹ë³„ë¡œ ì‹œê°í™”
def plot_churn_distribution_by_columns(df, columns, target_col='target', ncols=3, figsize=(18, 4)):
    """
    ì—¬ëŸ¬ ì»¬ëŸ¼ì— ëŒ€í•´ ì´íƒˆì(target=1) ë¶„í¬ë¥¼ ê·¸ë£¹ë³„ë¡œ ì‹œê°í™”

    Parameters:
    - df: DataFrame
    - columns: list[str] - ë¶„ì„í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    - target_col: str - íƒ€ê²Ÿ ë³€ìˆ˜ëª… (ê¸°ë³¸ê°’: 'target')
    - ncols: int - í•œ í–‰ë‹¹ subplot ê°œìˆ˜
    - figsize: tuple - ì „ì²´ ê·¸ë˜í”„ í¬ê¸°
    """
    n_plots = len(columns)
    nrows = math.ceil(n_plots / ncols)

    plt.figure(figsize=(figsize[0], figsize[1] * nrows))

    for idx, col in enumerate(columns):
        plt.subplot(nrows, ncols, idx + 1)
        sns.histplot(data=df, x=col, hue=target_col, multiple='stack', shrink=0.9, palette='coolwarm', bins=30)
        plt.title(f'{col} vs {target_col}', fontsize=10)
        plt.xlabel(col)
        plt.ylabel("Count")
        plt.grid(axis='y', linestyle='--', alpha=0.5)

    plt.tight_layout()
    plt.show()

######################################
# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ
######################################

# if __name__ == "__main__":
#     from sklearn.datasets import make_classification
#     from sklearn.model_selection import train_test_split
#     from sklearn.linear_model import LogisticRegression

#     X, y = make_classification(n_samples=500, n_features=5, n_informative=3, random_state=42)
#     df = pd.DataFrame(X, columns=['score_mean', 'click_sum', 'click_mean', 'dummy1', 'dummy2'])
#     df['target'] = y
#     df['gender'] = np.random.choice(['M', 'F'], size=500)
#     df['highest_education'] = np.random.choice(['HE', 'A Level', 'No Formal'], size=500)
#     df['final_result'] = np.random.choice(['Pass', 'Distinction', 'Fail', 'Withdrawn'], size=500)

#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#     model = LogisticRegression()
#     model.fit(X_train, y_train)
#     y_proba = model.predict_proba(X_test)[:, 1]
#     y_pred = model.predict(X_test)

#     # ëª¨ë¸ í‰ê°€ ì§€í‘œ ì‹œê°í™”
#     plot_precision_recall_curve(y_test, y_proba)
#     plot_roc_curve(y_test, y_proba)
#     plot_confusion_matrix(y_test, y_pred)

#     # ë°ì´í„° EDA ì‹œê°í™”
#     plot_final_result_distribution(df)
#     plot_feature_vs_target(df, 'score_mean')
#     plot_categorical_target_rate(df, 'gender')
#     plot_boxplot(df, ['score_mean', 'click_sum'])
#     plot_churn_rate_by_category(df, 'target', ['gender', 'highest_education'])
#     plot_correlation_heatmap(df)
